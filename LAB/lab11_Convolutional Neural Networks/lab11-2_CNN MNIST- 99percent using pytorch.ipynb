{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab11-2_CNN MNIST- 99percent\n",
    "lab11-1 에서 진행한 CNN에서 CONV 와 POOL layer 를 하나씩 더 준다면.\n",
    "![lec11_25](../../img/lec11_25.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MNIST data\n",
    "mnist_trainset = datasets.MNIST(root = './data',train =True,transform=transforms.ToTensor(),download=True)\n",
    "mnist_testset = datasets.MNIST(root = './data', train = False, transform=transforms.ToTensor(),download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = DataLoader(dataset=mnist_trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=mnist_testset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv layer 1\n",
    "\n",
    "fillter : [1,32] -> 각각 한장씩 32개 filter<br>\n",
    "kernel_size : 3\\*3짜리\n",
    "stride : 1 -> 한칸씩 이동<br>\n",
    "padding : 1 -> 각각 1씩 padding<br>\n",
    "relu : ReLU 적용<br>\n",
    "pool : kernel size : 1장 당 2 하나씩, strides : 2 로 두칸씩<br>\n",
    "![lec11_26](../../img/lec11_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv layer 2\n",
    "\n",
    "fillter : [32,64] -> 3\\*3 짜리 흑백 image 로 각각 한장씩 64개 filter\n",
    "<br>\n",
    "여기서, 앞의 Input 으로부터 depth 가 32이므로, 이를 받는다.\n",
    "<br>\n",
    "stride : 1 -> 한칸씩 이동<br>\n",
    "padding : 1<br>\n",
    "relu : ReLU 적용<br>\n",
    "pool : kernel size : 1장 당 2 하나씩, strides : 2 로 두칸씩<br>\n",
    "![lec11_27](../../img/lec11_27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected(FC, Dense) Layer\n",
    "Input : [7\\*7\\*64] -> [1\\* 3136] 의  vector data<br>\n",
    "Output_classes : 10<br>\n",
    "bias : True <br>\n",
    "Init method : Xavier_initializer<br>\n",
    "Cost : softmax<br>\n",
    "optimizer : Adam\n",
    "\n",
    "![lec11_28](../../img/lec11_28.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Layer #1\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(1,32,kernel_size = 3, stride = 1, padding =1),\n",
    "                                    nn.ReLU(),\n",
    "                                   nn.MaxPool2d(kernel_size = 2, stride =2)\n",
    "                                   )\n",
    "        # Layer #2\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(32,64,kernel_size = 3, stride = 1, padding =1),\n",
    "                                    nn.ReLU(),\n",
    "                                   nn.MaxPool2d(kernel_size = 2, stride =2)\n",
    "                                   )        \n",
    "        self.fc = nn.Linear(7*7*64,10,bias=True)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        output = self.layer1(x)\n",
    "        output = self.layer2(output)\n",
    "        output = output.view(in_size,-1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return F.log_softmax(output,dim=1)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0\tLoss: 0.245029\n",
      "Test set: Average loss: 0.0007, Accuracy: 9772/10000 (97%)\n",
      "\n",
      "\n",
      "Train Epoch: 1\tLoss: 0.062521\n",
      "Test set: Average loss: 0.0004, Accuracy: 9877/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 2\tLoss: 0.045249\n",
      "Test set: Average loss: 0.0004, Accuracy: 9878/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 3\tLoss: 0.037146\n",
      "Test set: Average loss: 0.0004, Accuracy: 9891/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 4\tLoss: 0.029526\n",
      "Test set: Average loss: 0.0003, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "\n",
      "Train Epoch: 5\tLoss: 0.025352\n",
      "Test set: Average loss: 0.0003, Accuracy: 9889/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 6\tLoss: 0.021623\n",
      "Test set: Average loss: 0.0003, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "\n",
      "Train Epoch: 7\tLoss: 0.017671\n",
      "Test set: Average loss: 0.0004, Accuracy: 9877/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 8\tLoss: 0.015612\n",
      "Test set: Average loss: 0.0004, Accuracy: 9873/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 9\tLoss: 0.012256\n",
      "Test set: Average loss: 0.0003, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "\n",
      "Train Epoch: 10\tLoss: 0.010198\n",
      "Test set: Average loss: 0.0004, Accuracy: 9884/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 11\tLoss: 0.010419\n",
      "Test set: Average loss: 0.0003, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "\n",
      "Train Epoch: 12\tLoss: 0.007960\n",
      "Test set: Average loss: 0.0003, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "\n",
      "Train Epoch: 13\tLoss: 0.006832\n",
      "Test set: Average loss: 0.0004, Accuracy: 9899/10000 (98%)\n",
      "\n",
      "\n",
      "Train Epoch: 14\tLoss: 0.004901\n",
      "Test set: Average loss: 0.0003, Accuracy: 9914/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(mnist_trainset) / batch_size)\n",
    "    model.train()\n",
    "\n",
    "    for idx, (batch_xs, batch_ys) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_xs)\n",
    "        loss = criterion(y_pred, batch_ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += loss / total_batch\n",
    "    print('\\nTrain Epoch: {}\\tLoss: {:.6f}'.format(epoch, avg_cost))\n",
    "    \n",
    "\n",
    "#         if idx % 10 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * idx / len(train_loader), loss.data[0]))\n",
    "    # TEST\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
