{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab09-1_NN for XOR\n",
    "### 2018.09.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Choi-seonyeol/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만약에, Logistic Regression 으로 XOR을 해보려고 하면??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.77111626 [[-0.754581 ]\n",
      " [ 1.1853131]]\n",
      "100 0.7144506 [[-0.30446178]\n",
      " [ 0.76478404]]\n",
      "200 0.69998676 [[-0.13016026]\n",
      " [ 0.44605976]]\n",
      "300 0.6953966 [[-0.04748783]\n",
      " [ 0.26113254]]\n",
      "400 0.6939202 [[-0.01035234]\n",
      " [ 0.15464158]]\n",
      "500 0.6934272 [[0.0046251 ]\n",
      " [0.09278671]]\n",
      "600 0.693254 [[0.00932752]\n",
      " [0.05642774]]\n",
      "700 0.69319 [[0.009611  ]\n",
      " [0.03477307]]\n",
      "800 0.693165 [[0.00825806]\n",
      " [0.02170005]]\n",
      "900 0.6931548 [[0.00651995]\n",
      " [0.01370084]]\n",
      "1000 0.6931505 [[0.00490611]\n",
      " [0.00874224]]\n",
      "1100 0.6931487 [[0.00358143]\n",
      " [0.00563073]]\n",
      "1200 0.69314784 [[0.00256156]\n",
      " [0.00365632]]\n",
      "1300 0.6931475 [[0.00180602]\n",
      " [0.00239085]]\n",
      "1400 0.6931473 [[0.00126017]\n",
      " [0.00157258]]\n",
      "1500 0.69314724 [[0.00087257]\n",
      " [0.00103946]]\n",
      "1600 0.6931471 [[0.00060065]\n",
      " [0.0006898 ]]\n",
      "1700 0.6931472 [[0.00041177]\n",
      " [0.00045938]]\n",
      "1800 0.6931472 [[0.00028127]\n",
      " [0.00030671]]\n",
      "1900 0.6931472 [[0.00019162]\n",
      " [0.00020521]]\n",
      "2000 0.6931472 [[0.00013028]\n",
      " [0.00013755]]\n",
      "2100 0.6931472 [[8.844605e-05]\n",
      " [9.232206e-05]]\n",
      "2200 0.6931472 [[5.9958038e-05]\n",
      " [6.2033949e-05]]\n",
      "2300 0.6931472 [[4.0608877e-05]\n",
      " [4.1717707e-05]]\n",
      "2400 0.6931472 [[2.7485417e-05]\n",
      " [2.8078681e-05]]\n",
      "2500 0.6931472 [[1.8586446e-05]\n",
      " [1.8901053e-05]]\n",
      "2600 0.6931472 [[1.2560423e-05]\n",
      " [1.2730484e-05]]\n",
      "2700 0.6931472 [[8.4909143e-06]\n",
      " [8.5700785e-06]]\n",
      "2800 0.6931472 [[5.731218e-06]\n",
      " [5.765683e-06]]\n",
      "2900 0.6931472 [[3.8730409e-06]\n",
      " [3.8926037e-06]]\n",
      "3000 0.6931472 [[2.6019748e-06]\n",
      " [2.6066355e-06]]\n",
      "3100 0.6931472 [[1.7779374e-06]\n",
      " [1.7766378e-06]]\n",
      "3200 0.6931472 [[1.1938095e-06]\n",
      " [1.1925098e-06]]\n",
      "3300 0.6931472 [[7.9743637e-07]\n",
      " [7.9613670e-07]]\n",
      "3400 0.6931471 [[5.0984340e-07]\n",
      " [5.0854374e-07]]\n",
      "3500 0.6931472 [[3.5785143e-07]\n",
      " [3.5655174e-07]]\n",
      "3600 0.6931471 [[2.2374047e-07]\n",
      " [2.2244078e-07]]\n",
      "3700 0.6931472 [[1.5817511e-07]\n",
      " [1.5687542e-07]]\n",
      "3800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "3900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4100 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4200 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4300 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4400 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4500 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4600 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4700 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "4900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5100 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5200 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5300 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5400 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5500 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5600 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5700 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "5900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6100 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6200 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6300 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6400 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6500 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6600 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6700 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "6900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7100 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7200 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7300 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7400 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7500 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7600 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7700 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "7900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8100 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8200 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8300 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8400 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8500 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8600 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8700 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "8900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9100 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9200 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9300 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9400 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9500 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9600 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9700 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9800 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "9900 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "10000 0.6931472 [[1.3284304e-07]\n",
      " [1.3154335e-07]]\n",
      "\n",
      "Hypothesis :  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2,1]),name= \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name = \"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "# cost / Loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype= tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),dtype= tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}), sess.run(W))\n",
    "        \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis : \",h, \"\\nCorrect : \",c,\"\\nAccuracy : \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 잘 안된다! 아무리 많이 돌려도, 0.5를 넘을 수 없다. -> 이론적으로, XOR은 logistic Regression 으로 불가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net을 이용한다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.81234115 [array([[-0.69583565, -0.20343342],\n",
      "       [-0.61689687, -1.5539503 ]], dtype=float32), array([[-1.061857 ],\n",
      "       [-1.1565485]], dtype=float32)]\n",
      "100 0.69149995 [array([[-0.7358825 , -0.42810738],\n",
      "       [-0.63187456, -1.5947356 ]], dtype=float32), array([[-0.62884086],\n",
      "       [-1.0161468 ]], dtype=float32)]\n",
      "200 0.68791234 [array([[-0.7191526 , -0.59301066],\n",
      "       [-0.59867734, -1.6212083 ]], dtype=float32), array([[-0.5711133],\n",
      "       [-1.0527254]], dtype=float32)]\n",
      "300 0.6841462 [array([[-0.70147914, -0.7562053 ],\n",
      "       [-0.5674566 , -1.6605014 ]], dtype=float32), array([[-0.5219943],\n",
      "       [-1.1182379]], dtype=float32)]\n",
      "400 0.67990756 [array([[-0.6838371 , -0.92047405],\n",
      "       [-0.5384695 , -1.7125684 ]], dtype=float32), array([[-0.47186393],\n",
      "       [-1.2074004 ]], dtype=float32)]\n",
      "500 0.67499226 [array([[-0.66629094, -1.0871665 ],\n",
      "       [-0.51138395, -1.7773094 ]], dtype=float32), array([[-0.41988742],\n",
      "       [-1.3184632 ]], dtype=float32)]\n",
      "600 0.6692039 [array([[-0.64899343, -1.2573932 ],\n",
      "       [-0.48610318, -1.8549087 ]], dtype=float32), array([[-0.36535943],\n",
      "       [-1.4499481 ]], dtype=float32)]\n",
      "700 0.66235644 [array([[-0.6322638 , -1.432074  ],\n",
      "       [-0.46279967, -1.9456782 ]], dtype=float32), array([[-0.30748495],\n",
      "       [-1.6004301 ]], dtype=float32)]\n",
      "800 0.654297 [array([[-0.616653  , -1.6118846 ],\n",
      "       [-0.44195637, -2.0498857 ]], dtype=float32), array([[-0.24541047],\n",
      "       [-1.7683151 ]], dtype=float32)]\n",
      "900 0.6449472 [array([[-0.6030214, -1.7971113],\n",
      "       [-0.4244224, -2.167545 ]], dtype=float32), array([[-0.17828795],\n",
      "       [-1.9515666 ]], dtype=float32)]\n",
      "1000 0.63435256 [array([[-0.5926198 , -1.9874415 ],\n",
      "       [-0.41147402, -2.298147  ]], dtype=float32), array([[-0.10534374],\n",
      "       [-2.1474648 ]], dtype=float32)]\n",
      "1100 0.622709 [array([[-0.5871484 , -2.1817818 ],\n",
      "       [-0.40484983, -2.4404104 ]], dtype=float32), array([[-0.02588396],\n",
      "       [-2.3525448 ]], dtype=float32)]\n",
      "1200 0.6103328 [array([[-0.5887596, -2.3782434],\n",
      "       [-0.4067274, -2.5921605]], dtype=float32), array([[ 0.06084785],\n",
      "       [-2.562822  ]], dtype=float32)]\n",
      "1300 0.597572 [array([[-0.59997076, -2.5743737 ],\n",
      "       [-0.41961798, -2.7504854 ]], dtype=float32), array([[ 0.15598331],\n",
      "       [-2.7742836 ]], dtype=float32)]\n",
      "1400 0.58469176 [array([[-0.6234707 , -2.767578  ],\n",
      "       [-0.44616523, -2.9121325 ]], dtype=float32), array([[ 0.26140136],\n",
      "       [-2.9834616 ]], dtype=float32)]\n",
      "1500 0.5717834 [array([[-0.6617617 , -2.955579  ],\n",
      "       [-0.48878622, -3.0740035 ]], dtype=float32), array([[ 0.37993917],\n",
      "       [-3.187892  ]], dtype=float32)]\n",
      "1600 0.55873984 [array([[-0.7165162 , -3.1367245 ],\n",
      "       [-0.54903513, -3.2335567 ]], dtype=float32), array([[ 0.51518154],\n",
      "       [-3.3863184 ]], dtype=float32)]\n",
      "1700 0.5453144 [array([[-0.78762054, -3.3100853 ],\n",
      "       [-0.62667054, -3.3890052 ]], dtype=float32), array([[ 0.6706345],\n",
      "       [-3.5786636]], dtype=float32)]\n",
      "1800 0.53124416 [array([[-0.8723908 , -3.475378  ],\n",
      "       [-0.71893305, -3.5393023 ]], dtype=float32), array([[ 0.8484227],\n",
      "       [-3.765794 ]], dtype=float32)]\n",
      "1900 0.5163305 [array([[-0.9660354 , -3.632773  ],\n",
      "       [-0.82111305, -3.684002  ]], dtype=float32), array([[ 1.0483375],\n",
      "       [-3.9491441]], dtype=float32)]\n",
      "2000 0.5003833 [array([[-1.063758 , -3.7826788],\n",
      "       [-0.9287347, -3.823045 ]], dtype=float32), array([[ 1.2681948],\n",
      "       [-4.1303024]], dtype=float32)]\n",
      "2100 0.48311228 [array([[-1.1630449, -3.9255478],\n",
      "       [-1.0397853, -3.9565728]], dtype=float32), array([[ 1.5052538],\n",
      "       [-4.310686 ]], dtype=float32)]\n",
      "2200 0.46412197 [array([[-1.2643946, -4.061778 ],\n",
      "       [-1.1551682, -4.084794 ]], dtype=float32), array([[ 1.7573702],\n",
      "       [-4.491366 ]], dtype=float32)]\n",
      "2300 0.44302186 [array([[-1.3703846, -4.1916466],\n",
      "       [-1.2773913, -4.2079053]], dtype=float32), array([[ 2.0231833],\n",
      "       [-4.6730475]], dtype=float32)]\n",
      "2400 0.41956952 [array([[-1.4840777, -4.3152957],\n",
      "       [-1.4086981, -4.3260317]], dtype=float32), array([[ 2.3016164],\n",
      "       [-4.8560495]], dtype=float32)]\n",
      "2500 0.3937934 [array([[-1.6075861, -4.4327583],\n",
      "       [-1.5497444, -4.439185 ]], dtype=float32), array([[ 2.5912075],\n",
      "       [-5.040304 ]], dtype=float32)]\n",
      "2600 0.36606497 [array([[-1.7412295, -4.5439863],\n",
      "       [-1.6992497, -4.5472436]], dtype=float32), array([[ 2.889596 ],\n",
      "       [-5.2253385]], dtype=float32)]\n",
      "2700 0.3370893 [array([[-1.8833878, -4.648904 ],\n",
      "       [-1.8544333, -4.649983 ]], dtype=float32), array([[ 3.193362 ],\n",
      "       [-5.4103036]], dtype=float32)]\n",
      "2800 0.30779898 [array([[-2.0309534, -4.747454 ],\n",
      "       [-2.0117896, -4.7471395]], dtype=float32), array([[ 3.4982574],\n",
      "       [-5.5940475]], dtype=float32)]\n",
      "2900 0.27917677 [array([[-2.1801374, -4.839644 ],\n",
      "       [-2.1678033, -4.8385057]], dtype=float32), array([[ 3.7997375],\n",
      "       [-5.775271 ]], dtype=float32)]\n",
      "3000 0.25207242 [array([[-2.3272936, -4.9255786],\n",
      "       [-2.3194566, -4.9239955]], dtype=float32), array([[ 4.0935783],\n",
      "       [-5.952695 ]], dtype=float32)]\n",
      "3100 0.22708479 [array([[-2.4694767, -5.0054655],\n",
      "       [-2.4644883, -5.003674 ]], dtype=float32), array([[ 4.376363],\n",
      "       [-6.12521 ]], dtype=float32)]\n",
      "3200 0.20453002 [array([[-2.6046758, -5.079608 ],\n",
      "       [-2.601451 , -5.077744 ]], dtype=float32), array([[ 4.64572  ],\n",
      "       [-6.2919664]], dtype=float32)]\n",
      "3300 0.18448155 [array([[-2.731765 , -5.148376 ],\n",
      "       [-2.7296271, -5.1465154]], dtype=float32), array([[ 4.9003243],\n",
      "       [-6.452394 ]], dtype=float32)]\n",
      "3400 0.16684163 [array([[-2.8503249, -5.2121763],\n",
      "       [-2.848859 , -5.210357 ]], dtype=float32), array([[ 5.1397204],\n",
      "       [-6.6061864]], dtype=float32)]\n",
      "3500 0.15141171 [array([[-2.960427 , -5.27144  ],\n",
      "       [-2.9593835, -5.269679 ]], dtype=float32), array([[ 5.364105],\n",
      "       [-6.753257]], dtype=float32)]\n",
      "3600 0.13794999 [array([[-3.0624464, -5.326574 ],\n",
      "       [-3.0616734, -5.324878 ]], dtype=float32), array([[ 5.5740905],\n",
      "       [-6.8936796]], dtype=float32)]\n",
      "3700 0.12620687 [array([[-3.1569233, -5.377969 ],\n",
      "       [-3.156327 , -5.3763404]], dtype=float32), array([[ 5.7705474],\n",
      "       [-7.0276494]], dtype=float32)]\n",
      "3800 0.11594678 [array([[-3.244465 , -5.425984 ],\n",
      "       [-3.2439868, -5.424421 ]], dtype=float32), array([[ 5.954456],\n",
      "       [-7.155435]], dtype=float32)]\n",
      "3900 0.106957644 [array([[-3.325693 , -5.4709487],\n",
      "       [-3.325295 , -5.4694467]], dtype=float32), array([[ 6.1268196],\n",
      "       [-7.277346 ]], dtype=float32)]\n",
      "4000 0.099054605 [array([[-3.4011998, -5.5131564],\n",
      "       [-3.4008589, -5.5117116]], dtype=float32), array([[ 6.288624],\n",
      "       [-7.393717]], dtype=float32)]\n",
      "4100 0.09207918 [array([[-3.471541 , -5.5528684],\n",
      "       [-3.4712381, -5.551482 ]], dtype=float32), array([[ 6.4407926],\n",
      "       [-7.5048833]], dtype=float32)]\n",
      "4200 0.085896954 [array([[-3.537217 , -5.59032  ],\n",
      "       [-3.5369463, -5.588983 ]], dtype=float32), array([[ 6.584185 ],\n",
      "       [-7.6111684]], dtype=float32)]\n",
      "4300 0.08039449 [array([[-3.5986853, -5.625718 ],\n",
      "       [-3.5984387, -5.624427 ]], dtype=float32), array([[ 6.7195787],\n",
      "       [-7.712902 ]], dtype=float32)]\n",
      "4400 0.075476505 [array([[-3.6563509, -5.659247 ],\n",
      "       [-3.6561234, -5.657997 ]], dtype=float32), array([[ 6.8476815],\n",
      "       [-7.8103795]], dtype=float32)]\n",
      "4500 0.07106292 [array([[-3.7105744, -5.691069 ],\n",
      "       [-3.7103615, -5.689856 ]], dtype=float32), array([[ 6.9691243],\n",
      "       [-7.90388  ]], dtype=float32)]\n",
      "4600 0.06708637 [array([[-3.7616758, -5.7213283],\n",
      "       [-3.7614753, -5.7201514]], dtype=float32), array([[ 7.0844774],\n",
      "       [-7.993667 ]], dtype=float32)]\n",
      "4700 0.06348975 [array([[-3.8099399, -5.7501545],\n",
      "       [-3.8097491, -5.7490115]], dtype=float32), array([[ 7.1942506],\n",
      "       [-8.079984 ]], dtype=float32)]\n",
      "4800 0.06022515 [array([[-3.8556168, -5.7776623],\n",
      "       [-3.855438 , -5.776551 ]], dtype=float32), array([[ 7.298895],\n",
      "       [-8.163052]], dtype=float32)]\n",
      "4900 0.057251383 [array([[-3.898934 , -5.8039546],\n",
      "       [-3.8987622, -5.8028755]], dtype=float32), array([[ 7.398829],\n",
      "       [-8.243085]], dtype=float32)]\n",
      "5000 0.054533806 [array([[-3.9400885, -5.829125 ],\n",
      "       [-3.9399226, -5.8280735]], dtype=float32), array([[ 7.494415 ],\n",
      "       [-8.3202715]], dtype=float32)]\n",
      "5100 0.052042607 [array([[-3.9792576, -5.8532543],\n",
      "       [-3.979097 , -5.852229 ]], dtype=float32), array([[ 7.5859814],\n",
      "       [-8.394786 ]], dtype=float32)]\n",
      "5200 0.049752146 [array([[-4.0166   , -5.876419 ],\n",
      "       [-4.016444 , -5.8754163]], dtype=float32), array([[ 7.673825],\n",
      "       [-8.466794]], dtype=float32)]\n",
      "5300 0.0476404 [array([[-4.0522566, -5.8986845],\n",
      "       [-4.0521083, -5.8977046]], dtype=float32), array([[ 7.7582145],\n",
      "       [-8.536439 ]], dtype=float32)]\n",
      "5400 0.045688264 [array([[-4.0863576, -5.920113 ],\n",
      "       [-4.086211 , -5.919155 ]], dtype=float32), array([[ 7.8393917],\n",
      "       [-8.603861 ]], dtype=float32)]\n",
      "5500 0.043879252 [array([[-4.1190147, -5.94076  ],\n",
      "       [-4.118872 , -5.9398217]], dtype=float32), array([[ 7.917572],\n",
      "       [-8.669187]], dtype=float32)]\n",
      "5600 0.042198792 [array([[-4.150333 , -5.9606752],\n",
      "       [-4.150193 , -5.959755 ]], dtype=float32), array([[ 7.9929566],\n",
      "       [-8.732534 ]], dtype=float32)]\n",
      "5700 0.040634293 [array([[-4.180404 , -5.979905 ],\n",
      "       [-4.1802707, -5.979001 ]], dtype=float32), array([[ 8.065727],\n",
      "       [-8.794012]], dtype=float32)]\n",
      "5800 0.03917466 [array([[-4.209315 , -5.99849  ],\n",
      "       [-4.2091837, -5.997604 ]], dtype=float32), array([[ 8.1360445],\n",
      "       [-8.85372  ]], dtype=float32)]\n",
      "5900 0.037810076 [array([[-4.2371416, -6.016469 ],\n",
      "       [-4.2370124, -6.0155997]], dtype=float32), array([[ 8.204064],\n",
      "       [-8.911751]], dtype=float32)]\n",
      "6000 0.03653195 [array([[-4.2639537, -6.033879 ],\n",
      "       [-4.2638264, -6.0330253]], dtype=float32), array([[ 8.26992 ],\n",
      "       [-8.968191]], dtype=float32)]\n",
      "6100 0.035332642 [array([[-4.2898154, -6.0507503],\n",
      "       [-4.2896905, -6.0499105]], dtype=float32), array([[ 8.33374 ],\n",
      "       [-9.023124]], dtype=float32)]\n",
      "6200 0.034205273 [array([[-4.3147855, -6.067113 ],\n",
      "       [-4.314662 , -6.066288 ]], dtype=float32), array([[ 8.395638],\n",
      "       [-9.076619]], dtype=float32)]\n",
      "6300 0.033143867 [array([[-4.3389173, -6.082995 ],\n",
      "       [-4.3387947, -6.0821843]], dtype=float32), array([[ 8.455727],\n",
      "       [-9.128749]], dtype=float32)]\n",
      "6400 0.032142963 [array([[-4.362259 , -6.0984225],\n",
      "       [-4.3621373, -6.0976243]], dtype=float32), array([[ 8.514097],\n",
      "       [-9.179578]], dtype=float32)]\n",
      "6500 0.031197665 [array([[-4.3848557, -6.113418 ],\n",
      "       [-4.384735 , -6.1126323]], dtype=float32), array([[ 8.570843],\n",
      "       [-9.229165]], dtype=float32)]\n",
      "6600 0.03030373 [array([[-4.406749 , -6.1280036],\n",
      "       [-4.4066305, -6.1272283]], dtype=float32), array([[ 8.626048],\n",
      "       [-9.27757 ]], dtype=float32)]\n",
      "6700 0.029457076 [array([[-4.427978 , -6.142199 ],\n",
      "       [-4.4278617, -6.141435 ]], dtype=float32), array([[ 8.679791],\n",
      "       [-9.324841]], dtype=float32)]\n",
      "6800 0.028654272 [array([[-4.4485774, -6.156023 ],\n",
      "       [-4.4484625, -6.155271 ]], dtype=float32), array([[ 8.732143],\n",
      "       [-9.371032]], dtype=float32)]\n",
      "6900 0.027892007 [array([[-4.468581 , -6.1694946],\n",
      "       [-4.468468 , -6.1687527]], dtype=float32), array([[ 8.783175],\n",
      "       [-9.416187]], dtype=float32)]\n",
      "7000 0.027167428 [array([[-4.4880185, -6.182629 ],\n",
      "       [-4.4879065, -6.181897 ]], dtype=float32), array([[ 8.832947],\n",
      "       [-9.460351]], dtype=float32)]\n",
      "7100 0.026477901 [array([[-4.506919 , -6.195443 ],\n",
      "       [-4.5068083, -6.1947203]], dtype=float32), array([[ 8.8815155],\n",
      "       [-9.503566 ]], dtype=float32)]\n",
      "7200 0.025821008 [array([[-4.5253086, -6.207948 ],\n",
      "       [-4.5251985, -6.2072353]], dtype=float32), array([[ 8.928938],\n",
      "       [-9.545868]], dtype=float32)]\n",
      "7300 0.025194518 [array([[-4.54321  , -6.220161 ],\n",
      "       [-4.5431013, -6.219457 ]], dtype=float32), array([[ 8.975264],\n",
      "       [-9.587295]], dtype=float32)]\n",
      "7400 0.024596397 [array([[-4.560648 , -6.232093 ],\n",
      "       [-4.5605397, -6.2313976]], dtype=float32), array([[ 9.020545],\n",
      "       [-9.627881]], dtype=float32)]\n",
      "7500 0.024024889 [array([[-4.5776434, -6.243754 ],\n",
      "       [-4.577536 , -6.243067 ]], dtype=float32), array([[ 9.064821],\n",
      "       [-9.667657]], dtype=float32)]\n",
      "7600 0.023478288 [array([[-4.5942173, -6.2551584],\n",
      "       [-4.5941105, -6.2544785]], dtype=float32), array([[ 9.108139 ],\n",
      "       [-9.7066555]], dtype=float32)]\n",
      "7700 0.022954963 [array([[-4.6103864, -6.2663145],\n",
      "       [-4.6102805, -6.2656426]], dtype=float32), array([[ 9.150533 ],\n",
      "       [-9.7449045]], dtype=float32)]\n",
      "7800 0.022453561 [array([[-4.6261697, -6.2772326],\n",
      "       [-4.626065 , -6.276568 ]], dtype=float32), array([[ 9.192041],\n",
      "       [-9.782433]], dtype=float32)]\n",
      "7900 0.02197279 [array([[-4.641584 , -6.2879214],\n",
      "       [-4.641481 , -6.2872643]], dtype=float32), array([[ 9.232701],\n",
      "       [-9.819265]], dtype=float32)]\n",
      "8000 0.021511355 [array([[-4.656643 , -6.2983913],\n",
      "       [-4.6565404, -6.297743 ]], dtype=float32), array([[ 9.272547],\n",
      "       [-9.855426]], dtype=float32)]\n",
      "8100 0.02106822 [array([[-4.671363 , -6.308649 ],\n",
      "       [-4.671261 , -6.3080072]], dtype=float32), array([[ 9.311607],\n",
      "       [-9.890938]], dtype=float32)]\n",
      "8200 0.020642286 [array([[-4.685757 , -6.318704 ],\n",
      "       [-4.685657 , -6.3180685]], dtype=float32), array([[ 9.349913],\n",
      "       [-9.925827]], dtype=float32)]\n",
      "8300 0.020232622 [array([[-4.6998377, -6.3285627],\n",
      "       [-4.6997385, -6.3279333]], dtype=float32), array([[ 9.38749 ],\n",
      "       [-9.960112]], dtype=float32)]\n",
      "8400 0.019838288 [array([[-4.7136188, -6.3382316],\n",
      "       [-4.71352  , -6.3376093]], dtype=float32), array([[ 9.424366],\n",
      "       [-9.993812]], dtype=float32)]\n",
      "8500 0.019458504 [array([[-4.72711  , -6.3477182],\n",
      "       [-4.727012 , -6.3471026]], dtype=float32), array([[  9.460567],\n",
      "       [-10.026947]], dtype=float32)]\n",
      "8600 0.019092478 [array([[-4.7403235, -6.3570304],\n",
      "       [-4.7402267, -6.35642  ]], dtype=float32), array([[  9.496119],\n",
      "       [-10.059532]], dtype=float32)]\n",
      "8700 0.018739551 [array([[-4.7532697, -6.36617  ],\n",
      "       [-4.753173 , -6.365566 ]], dtype=float32), array([[  9.531035],\n",
      "       [-10.091591]], dtype=float32)]\n",
      "8800 0.018398952 [array([[-4.7659583, -6.375148 ],\n",
      "       [-4.765862 , -6.374549 ]], dtype=float32), array([[  9.565346],\n",
      "       [-10.123134]], dtype=float32)]\n",
      "8900 0.0180701 [array([[-4.7783966, -6.383965 ],\n",
      "       [-4.7783012, -6.383373 ]], dtype=float32), array([[  9.59907],\n",
      "       [-10.15418]], dtype=float32)]\n",
      "9000 0.017752409 [array([[-4.790595 , -6.3926287],\n",
      "       [-4.7905016, -6.3920417]], dtype=float32), array([[  9.632224],\n",
      "       [-10.184745]], dtype=float32)]\n",
      "9100 0.017445348 [array([[-4.8025627, -6.401144 ],\n",
      "       [-4.8024693, -6.4005623]], dtype=float32), array([[  9.664828],\n",
      "       [-10.214843]], dtype=float32)]\n",
      "9200 0.01714838 [array([[-4.814306, -6.409516],\n",
      "       [-4.814213, -6.408939]], dtype=float32), array([[  9.696902],\n",
      "       [-10.244488]], dtype=float32)]\n",
      "9300 0.016861005 [array([[-4.8258348, -6.4177485],\n",
      "       [-4.825742 , -6.4171762]], dtype=float32), array([[  9.728457],\n",
      "       [-10.273689]], dtype=float32)]\n",
      "9400 0.016582837 [array([[-4.837153 , -6.4258466],\n",
      "       [-4.8370605, -6.4252796]], dtype=float32), array([[  9.759513],\n",
      "       [-10.302466]], dtype=float32)]\n",
      "9500 0.01631341 [array([[-4.8482695, -6.433812 ],\n",
      "       [-4.8481784, -6.4332485]], dtype=float32), array([[  9.790085],\n",
      "       [-10.330823]], dtype=float32)]\n",
      "9600 0.016052356 [array([[-4.859191 , -6.4416504],\n",
      "       [-4.859101 , -6.4410925]], dtype=float32), array([[  9.820188],\n",
      "       [-10.358777]], dtype=float32)]\n",
      "9700 0.015799195 [array([[-4.8699236, -6.449368 ],\n",
      "       [-4.869834 , -6.448814 ]], dtype=float32), array([[  9.849836],\n",
      "       [-10.386338]], dtype=float32)]\n",
      "9800 0.01555371 [array([[-4.8804736, -6.456962 ],\n",
      "       [-4.8803844, -6.456413 ]], dtype=float32), array([[  9.879038],\n",
      "       [-10.413517]], dtype=float32)]\n",
      "9900 0.01531553 [array([[-4.8908453, -6.4644427],\n",
      "       [-4.8907566, -6.463897 ]], dtype=float32), array([[  9.907814],\n",
      "       [-10.440323]], dtype=float32)]\n",
      "10000 0.015084287 [array([[-4.9010444, -6.4718094],\n",
      "       [-4.9009566, -6.471269 ]], dtype=float32), array([[  9.936168],\n",
      "       [-10.466769]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.01186781]\n",
      " [0.98651636]\n",
      " [0.98651695]\n",
      " [0.02102404]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype= tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),dtype= tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확하게 예측한다! logistic regression 을 두개 연결하여 아주간단한 neural network를 만들었음에도, 굉장히 잘 학습된 모델이 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide NN for XOR\n",
    "\n",
    "그렇다면, 이를 조금 더 넓고 깊게 학습시킨다면? ( 각 layer에서 더 많은 node로 서로 연결한다면 )\n",
    "\n",
    "![lec09_8](../../img/lec09_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "~~~\n",
    "\n",
    "이렇게, 처음 Input은 2지만 중간의 2개의 hidden layer에서 node를 10개씩 처리한 후 최종적으로 1개의 output을 낸다면?\n",
    "--> 중간에 Hidden Layer 를 만들어 deep!\n",
    "--> 노드의 개수를 증가시켜 wide !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훨신 더 좋은 성능이 나오게 된다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
