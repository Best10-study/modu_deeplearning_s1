{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab07-1 Learning rate, Evaluation using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1],[1, 3, 2],[1, 3, 4],[1, 5, 5],[1, 7, 5],[1, 2, 5],[1, 6, 6],[1, 7, 7]]\n",
    "y_data = [[0, 0, 1],[0, 0, 1],[0, 0, 1],[0, 1, 0],[0, 1, 0],[0, 1, 0],[1, 0, 0],[1, 0, 0]]\n",
    "\n",
    "x_test = [[2, 1, 1],[3, 1, 2],[3, 3, 4]]\n",
    "y_test = [[0, 0, 1],[0, 0, 1],[0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = Variable(torch.Tensor(x_data))\n",
    "Y = Variable(torch.Tensor(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(3, 3,bias = True) # 2 in and 1 out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3)\n",
    "        y_pred = self.linear(x)\n",
    "        F.softmax(y_pred,dim=1)\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8746886 \n",
      " [[-0.03547042  0.31727114  0.35857692]\n",
      " [-0.488123    0.13634145  0.32538578]\n",
      " [ 0.14158906  0.42832777  0.3866726 ]] [-0.44367993 -0.38319623 -0.10424625]\n",
      "200 0.88123804 \n",
      " [[-0.23530143  0.35226262  0.41702658]\n",
      " [-0.47984898  0.23375244  0.5541126 ]\n",
      " [ 0.3331462   0.2959253   0.09949615]] [-0.6435105  -0.37492222  0.08731066]\n",
      "400 0.81042296 \n",
      " [[-0.40408772  0.3729268   0.46534777]\n",
      " [-0.49986315  0.21025321  0.59062827]\n",
      " [ 0.52194643  0.29876038  0.01465949]] [-0.8122963  -0.39493638  0.2761108 ]\n",
      "600 0.75966495 \n",
      " [[-0.54965013  0.3888038   0.5085117 ]\n",
      " [-0.51566607  0.2042124   0.608061  ]\n",
      " [ 0.68331176  0.28892392 -0.04593743]] [-0.9578589  -0.41073912  0.4374764 ]\n",
      "800 0.72100914 \n",
      " [[-0.67857236  0.4014566   0.54777694]\n",
      " [-0.5272222   0.20642616  0.6153979 ]\n",
      " [ 0.82379     0.27405733 -0.0925395 ]] [-1.0867805 -0.4222953  0.5779543]\n",
      "1000 0.69022024 \n",
      " [[-0.7951992   0.4117002   0.58422965]\n",
      " [-0.53496736  0.21221988  0.6174265 ]\n",
      " [ 0.94816196  0.25801983 -0.13102032]] [-1.2034078  -0.43004063  0.70232624]\n",
      "1200 0.66484946 \n",
      " [[-0.9024337   0.4200259   0.6186347 ]\n",
      " [-0.5394487   0.21927726  0.61666644]\n",
      " [ 1.059877    0.24263681 -0.16466455]] [-1.3106422 -0.4345216  0.8140417]\n",
      "1400 0.643377 \n",
      " [[-1.0022739   0.4267768   0.6514981 ]\n",
      " [-0.54119354  0.22649772  0.6144281 ]\n",
      " [ 1.1614611   0.22866538 -0.19528976]] [-1.4104818  -0.43626717  0.9156266 ]\n",
      "1600 0.6248066 \n",
      " [[-1.0961388   0.43222225  0.6831467 ]\n",
      " [-0.54067045  0.23339519  0.61139137]\n",
      " [ 1.2548035   0.21632269 -0.22390208]] [-1.5043467  -0.43574414  1.0089684 ]\n",
      "1800 0.60845923 \n",
      " [[-1.1850673   0.43658444  0.71379167]\n",
      " [-0.5382763   0.2397863   0.60790884]\n",
      " [ 1.3413384   0.20556961 -0.25106466]] [-1.5932752  -0.43334997  1.0955032 ]\n",
      "2000 0.5938561 \n",
      " [[-1.2698419   0.44005147  0.74357146]\n",
      " [-0.5343425   0.24563085  0.6041661 ]\n",
      " [ 1.4221795   0.1962581  -0.2771019 ]] [-1.6780499  -0.42941624  1.1763443 ]\n",
      "2200 0.5806522 \n",
      " [[-1.3510606   0.44278204  0.77257895]\n",
      " [-0.5291433   0.25095192  0.600264  ]\n",
      " [ 1.4982005   0.18820646 -0.30220804]] [-1.7592686 -0.4242172  1.2523654]\n",
      "2400 0.568592 \n",
      " [[-1.4291933   0.44490933  0.80087966]\n",
      " [-0.5229054   0.25579607  0.5962605 ]\n",
      " [ 1.5700952   0.18123479 -0.3265063 ]] [-1.8374013 -0.4179794  1.32426  ]\n",
      "2600 0.5574825 \n",
      " [[-1.5046109   0.44654432  0.828522  ]\n",
      " [-0.5158162   0.2602164   0.59219235]\n",
      " [ 1.6384242   0.17517956 -0.3500812 ]] [-1.9128189  -0.41089064  1.392589  ]\n",
      "2800 0.5471761 \n",
      " [[-1.5776134   0.4477787   0.85554373]\n",
      " [-0.5080316   0.26426312  0.5880838 ]\n",
      " [ 1.7036427   0.16989847 -0.37299496]] [-1.9858214  -0.40310597  1.4578075 ]\n",
      "3000 0.53755677 \n",
      " [[-1.6484474   0.44868776  0.881976  ]\n",
      " [-0.49968097  0.26798224  0.58395237]\n",
      " [ 1.7661253   0.16527022 -0.39529753]] [-2.0566552  -0.39475533  1.5202901 ]\n",
      "3200 0.528533 \n",
      " [[-1.7173144   0.44933432  0.907847  ]\n",
      " [-0.49087274  0.2714122   0.5798158 ]\n",
      " [ 1.826185    0.16119377 -0.417029  ]] [-2.1255226 -0.3859471  1.5803498]\n",
      "3400 0.52003026 \n",
      " [[-1.7843841   0.449767    0.93317944]\n",
      " [-0.48169753  0.27458954  0.57568014]\n",
      " [ 1.8840797   0.15758356 -0.438226  ]] [-2.1925926 -0.3767719  1.6382445]\n",
      "3600 0.51198876 \n",
      " [[-1.8497992   0.45002845  0.957996  ]\n",
      " [-0.47223204  0.2775415   0.5715592 ]\n",
      " [ 1.9400295   0.15436974 -0.45892027]] [-2.258009  -0.3673064  1.6941943]\n",
      "3800 0.50435865 \n",
      " [[-1.9136822   0.4501517   0.98231673]\n",
      " [-0.46254095  0.2802941   0.56745946]\n",
      " [ 1.9942216   0.15149358 -0.4791409 ]] [-2.3218918  -0.35761532  1.7483864 ]\n",
      "4000 0.49709806 \n",
      " [[-1.9761386   0.45016456  1.006161  ]\n",
      " [-0.45267913  0.2828684   0.56338906]\n",
      " [ 2.046816    0.14890596 -0.4989142 ]] [-2.3843472 -0.3477535  1.8009808]\n",
      "4200 0.49017203 \n",
      " [[-2.037259    0.4500893   1.0295471 ]\n",
      " [-0.4426932   0.28528273  0.5593537 ]\n",
      " [ 2.097951    0.1465664  -0.5182646 ]] [-2.4454675  -0.33776757  1.8521147 ]\n",
      "4400 0.48355043 \n",
      " [[-2.0971227   0.4499442   1.0524915 ]\n",
      " [-0.4326225   0.2875532   0.55535847]\n",
      " [ 2.1477442   0.14444076 -0.5372144 ]] [-2.505331   -0.32769683  1.9019082 ]\n",
      "4600 0.47720766 \n",
      " [[-2.1558022   0.44974437  1.075012  ]\n",
      " [-0.42250088  0.28969333  0.5514083 ]\n",
      " [ 2.1963015   0.14250042 -0.5557842 ]] [-2.5640106  -0.31757522  1.9504652 ]\n",
      "4800 0.47112146 \n",
      " [[-2.2133589   0.44950193  1.0971228 ]\n",
      " [-0.4123576   0.29171482  0.5475069 ]\n",
      " [ 2.2437143   0.14072089 -0.57399374]] [-2.6215672  -0.30743194  1.997878  ]\n",
      "5000 0.46527192 \n",
      " [[-2.26985     0.4492266   1.1188401 ]\n",
      " [-0.40221637  0.29362872  0.54365677]\n",
      " [ 2.290064    0.13908209 -0.59186083]] [-2.6780584 -0.2972907  2.0442276]\n",
      "5200 0.4596423 \n",
      " [[-2.3253255   0.4489266   1.1401767 ]\n",
      " [-0.3920979   0.2954434   0.53986096]\n",
      " [ 2.3354206   0.13756652 -0.60940176]] [-2.7335339  -0.28717223  2.089584  ]\n",
      "5400 0.45421714 \n",
      " [[-2.3798318   0.44860914  1.1611466 ]\n",
      " [-0.38202035  0.2971676   0.53612137]\n",
      " [ 2.3798494   0.13615975 -0.6266323 ]] [-2.7880402 -0.2770947  2.134013 ]\n",
      "5600 0.44898298 \n",
      " [[-2.4334111   0.44827896  1.1817632 ]\n",
      " [-0.37199882  0.29880813  0.53243965]\n",
      " [ 2.4234068   0.13484885 -0.6435668 ]] [-2.8416195  -0.26707315  2.1775703 ]\n",
      "5800 0.4439274 \n",
      " [[-2.4861014   0.44794112  1.2020375 ]\n",
      " [-0.36204588  0.30037147  0.52881706]\n",
      " [ 2.4661448   0.13362297 -0.6602187 ]] [-2.8943098  -0.25712022  2.2203083 ]\n",
      "6000 0.43903986 \n",
      " [[-2.5379398   0.44759938  1.2219818 ]\n",
      " [-0.35217255  0.30186337  0.5252545 ]\n",
      " [ 2.5081096   0.13247307 -0.6766005 ]] [-2.9461482  -0.24724686  2.262273  ]\n",
      "6200 0.4343099 \n",
      " [[-2.5889573   0.44725552  1.2416074 ]\n",
      " [-0.34238788  0.30328858  0.52175254]\n",
      " [ 2.5493433   0.13139068 -0.69272447]] [-2.9971657  -0.23746225  2.3035069 ]\n",
      "6400 0.42972907 \n",
      " [[-2.6391857   0.44691354  1.2609242 ]\n",
      " [-0.33269954  0.30465198  0.51831174]\n",
      " [ 2.589883    0.13036953 -0.708601  ]] [-3.047394   -0.22777386  2.3440466 ]\n",
      "6600 0.4252885 \n",
      " [[-2.6886537   0.4465742   1.2799435 ]\n",
      " [-0.32311386  0.30595735  0.51493233]\n",
      " [ 2.6297655   0.12940325 -0.7242409 ]] [-3.096862   -0.21818814  2.383929  ]\n",
      "6800 0.42098135 \n",
      " [[-2.7373874   0.44623935  1.298674  ]\n",
      " [-0.31363606  0.3072087   0.511614  ]\n",
      " [ 2.6690207   0.12848663 -0.73965365]] [-3.1455958  -0.20871031  2.4231842 ]\n",
      "7000 0.41680017 \n",
      " [[-2.7854106   0.44590992  1.3171251 ]\n",
      " [-0.3042705   0.30840948  0.5083566 ]\n",
      " [ 2.707679    0.12761503 -0.7548482 ]] [-3.193619   -0.19934464  2.4618425 ]\n",
      "7200 0.41273874 \n",
      " [[-2.832749    0.4455872   1.3353059 ]\n",
      " [-0.29502055  0.3095624   0.5051601 ]\n",
      " [ 2.7457669   0.1267846  -0.7698333 ]] [-3.2409573 -0.1900946  2.4999304]\n",
      "7400 0.40879115 \n",
      " [[-2.8794234   0.4452723   1.353225  ]\n",
      " [-0.2858889   0.3106706   0.50202453]\n",
      " [ 2.7833104   0.12599175 -0.78461677]] [-3.287632   -0.18096289  2.537474  ]\n",
      "7600 0.40495178 \n",
      " [[-2.925456    0.44496495  1.3708901 ]\n",
      " [-0.27687767  0.31173623  0.49894878]\n",
      " [ 2.8203313   0.12523317 -0.79920655]] [-3.3336647  -0.17195173  2.5744948 ]\n",
      "7800 0.40121573 \n",
      " [[-2.9708655   0.44466555  1.3883092 ]\n",
      " [-0.26798844  0.312762    0.49593192]\n",
      " [ 2.8568518   0.12450637 -0.81360984]] [-3.379074   -0.16306253  2.6110153 ]\n",
      "8000 0.39757836 \n",
      " [[-3.0156713   0.44437498  1.4054893 ]\n",
      " [-0.2592223   0.31374994  0.49297407]\n",
      " [ 2.8928912   0.12380911 -0.8278331 ]] [-3.4238799  -0.15429632  2.6470547 ]\n",
      "8200 0.3940348 \n",
      " [[-3.0598907   0.44409263  1.4224379 ]\n",
      " [-0.2505798   0.31470186  0.49007404]\n",
      " [ 2.9284692   0.12313888 -0.8418832 ]] [-3.4680994  -0.14565389  2.6826327 ]\n",
      "8400 0.39058113 \n",
      " [[-3.1035426   0.44381946  1.4391619 ]\n",
      " [-0.24206151  0.31561965  0.4872316 ]\n",
      " [ 2.9636016   0.12249412 -0.85576576]] [-3.5117512  -0.13713552  2.717765  ]\n",
      "8600 0.38721356 \n",
      " [[-3.1466408   0.44355506  1.4556675 ]\n",
      " [-0.23366733  0.31650516  0.48444572]\n",
      " [ 2.9983063   0.12187301 -0.8694866 ]] [-3.5548494  -0.12874135  2.7524698 ]\n",
      "8800 0.3839285 \n",
      " [[-3.189202    0.44329923  1.4719611 ]\n",
      " [-0.22539698  0.31736004  0.48171523]\n",
      " [ 3.0325968   0.12127382 -0.88305104]] [-3.5974107  -0.12047097  2.7867603 ]\n",
      "9000 0.38072214 \n",
      " [[-3.2312415   0.44305184  1.4880497 ]\n",
      " [-0.21724987  0.3181859   0.4790398 ]\n",
      " [ 3.0664895   0.12069552 -0.8964642 ]] [-3.63945    -0.11232389  2.820653  ]\n",
      "9200 0.37759167 \n",
      " [[-3.272773    0.44281295  1.5039375 ]\n",
      " [-0.20922539  0.31898382  0.47641814]\n",
      " [ 3.0999968   0.12013655 -0.9097314 ]] [-3.6809816  -0.10429941  2.8541603 ]\n",
      "9400 0.37453398 \n",
      " [[-3.3138108   0.4425827   1.5196306 ]\n",
      " [-0.2013227   0.31975493  0.47384953]\n",
      " [ 3.1331313   0.11959595 -0.92285717]] [-3.7220194  -0.09639676  2.8872948 ]\n",
      "9600 0.3715459 \n",
      " [[-3.3543668   0.44236016  1.5351347 ]\n",
      " [-0.19354084  0.32050052  0.47133303]\n",
      " [ 3.1659057   0.11907265 -0.93584573]] [-3.7625754  -0.08861482  2.9200692 ]\n",
      "9800 0.36862507 \n",
      " [[-3.394454    0.4421458   1.5504545 ]\n",
      " [-0.18587853  0.3212219   0.46886766]\n",
      " [ 3.198331    0.11856566 -0.94870156]] [-3.8026626  -0.08095253  2.9524946 ]\n",
      "10000 0.3657687 \n",
      " [[-3.4340856   0.44193876  1.5655959 ]\n",
      " [-0.17833473  0.32192022  0.46645272]\n",
      " [ 3.2304182   0.11807437 -0.96142864]] [-3.8422942  -0.07340868  2.9845817 ]\n"
     ]
    }
   ],
   "source": [
    "for t in range(10001):    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    cost = criterion(y_pred,torch.max(Y, 1)[1])\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if t % 200 == 0:\n",
    "        print(t,cost.data.numpy(),\"\\n\",list(model.parameters())[0].data.numpy(),list(model.parameters())[1].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = torch.max((F.softmax(model(torch.Tensor(x_test)),dim=1)),1)[1].float()\n",
    "correct_pred = (pred.numpy() == np.array(y_test).argmax(axis =1))\n",
    "accuracy = correct_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate  실험\n",
    "### CASE 1: learning rate  : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 \tcost :  1.0517436\n",
      "step :  200 \tcost :  80.18485\n",
      "step :  400 \tcost :  71.92984\n",
      "step :  600 \tcost :  53.277527\n",
      "step :  800 \tcost :  1.0728836e-05\n",
      "step :  1000 \tcost :  4.6491623e-06\n",
      "step :  1200 \tcost :  2.9802322e-06\n",
      "step :  1400 \tcost :  2.1457672e-06\n",
      "step :  1600 \tcost :  1.66893e-06\n",
      "step :  1800 \tcost :  1.4305115e-06\n",
      "step :  2000 \tcost :  1.1920929e-06\n",
      "step :  2200 \tcost :  9.536743e-07\n",
      "step :  2400 \tcost :  9.536743e-07\n",
      "step :  2600 \tcost :  7.1525574e-07\n",
      "step :  2800 \tcost :  7.1525574e-07\n",
      "step :  3000 \tcost :  7.1525574e-07\n",
      "step :  3200 \tcost :  7.1525574e-07\n",
      "step :  3400 \tcost :  4.7683716e-07\n",
      "step :  3600 \tcost :  4.7683716e-07\n",
      "step :  3800 \tcost :  4.7683716e-07\n",
      "step :  4000 \tcost :  4.7683716e-07\n",
      "step :  4200 \tcost :  4.7683716e-07\n",
      "step :  4400 \tcost :  4.7683716e-07\n",
      "step :  4600 \tcost :  4.7683716e-07\n",
      "step :  4800 \tcost :  4.7683716e-07\n",
      "step :  5000 \tcost :  4.7683716e-07\n",
      "step :  5200 \tcost :  2.3841858e-07\n",
      "step :  5400 \tcost :  2.3841858e-07\n",
      "step :  5600 \tcost :  2.3841858e-07\n",
      "step :  5800 \tcost :  2.3841858e-07\n",
      "step :  6000 \tcost :  2.3841858e-07\n",
      "step :  6200 \tcost :  2.3841858e-07\n",
      "step :  6400 \tcost :  2.3841858e-07\n",
      "step :  6600 \tcost :  2.3841858e-07\n",
      "step :  6800 \tcost :  2.3841858e-07\n",
      "step :  7000 \tcost :  2.3841858e-07\n",
      "step :  7200 \tcost :  2.3841858e-07\n",
      "step :  7400 \tcost :  2.3841858e-07\n",
      "step :  7600 \tcost :  2.3841858e-07\n",
      "step :  7800 \tcost :  2.3841858e-07\n",
      "step :  8000 \tcost :  2.3841858e-07\n",
      "step :  8200 \tcost :  2.3841858e-07\n",
      "step :  8400 \tcost :  2.3841858e-07\n",
      "step :  8600 \tcost :  2.3841858e-07\n",
      "step :  8800 \tcost :  2.3841858e-07\n",
      "step :  9000 \tcost :  2.3841858e-07\n",
      "step :  9200 \tcost :  2.3841858e-07\n",
      "step :  9400 \tcost :  2.3841858e-07\n",
      "step :  9600 \tcost :  2.3841858e-07\n",
      "step :  9800 \tcost :  2.3841858e-07\n",
      "step :  10000 \tcost :  2.3841858e-07\n"
     ]
    }
   ],
   "source": [
    "for t in range(10001):    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    cost = criterion(y_pred,torch.max(Y, 1)[1])\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if t % 200 == 0:\n",
    "        print(\"step : \",t,\"\\tcost : \",cost.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate  실험\n",
    "### CASE 2: learning rate  : 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 \tcost :  2.260271\n",
      "step :  200 \tcost :  2.260271\n",
      "step :  400 \tcost :  2.260271\n",
      "step :  600 \tcost :  2.260271\n",
      "step :  800 \tcost :  2.260271\n",
      "step :  1000 \tcost :  2.260271\n",
      "step :  1200 \tcost :  2.260271\n",
      "step :  1400 \tcost :  2.260271\n",
      "step :  1600 \tcost :  2.260271\n",
      "step :  1800 \tcost :  2.260271\n",
      "step :  2000 \tcost :  2.260271\n",
      "step :  2200 \tcost :  2.260271\n",
      "step :  2400 \tcost :  2.260271\n",
      "step :  2600 \tcost :  2.260271\n",
      "step :  2800 \tcost :  2.260271\n",
      "step :  3000 \tcost :  2.260271\n",
      "step :  3200 \tcost :  2.260271\n",
      "step :  3400 \tcost :  2.260271\n",
      "step :  3600 \tcost :  2.260271\n",
      "step :  3800 \tcost :  2.260271\n",
      "step :  4000 \tcost :  2.260271\n",
      "step :  4200 \tcost :  2.260271\n",
      "step :  4400 \tcost :  2.260271\n",
      "step :  4600 \tcost :  2.260271\n",
      "step :  4800 \tcost :  2.260271\n",
      "step :  5000 \tcost :  2.260271\n",
      "step :  5200 \tcost :  2.260271\n",
      "step :  5400 \tcost :  2.260271\n",
      "step :  5600 \tcost :  2.260271\n",
      "step :  5800 \tcost :  2.260271\n",
      "step :  6000 \tcost :  2.260271\n",
      "step :  6200 \tcost :  2.260271\n",
      "step :  6400 \tcost :  2.260271\n",
      "step :  6600 \tcost :  2.260271\n",
      "step :  6800 \tcost :  2.260271\n",
      "step :  7000 \tcost :  2.260271\n",
      "step :  7200 \tcost :  2.260271\n",
      "step :  7400 \tcost :  2.260271\n",
      "step :  7600 \tcost :  2.260271\n",
      "step :  7800 \tcost :  2.260271\n",
      "step :  8000 \tcost :  2.260271\n",
      "step :  8200 \tcost :  2.260271\n",
      "step :  8400 \tcost :  2.260271\n",
      "step :  8600 \tcost :  2.260271\n",
      "step :  8800 \tcost :  2.260271\n",
      "step :  9000 \tcost :  2.260271\n",
      "step :  9200 \tcost :  2.260271\n",
      "step :  9400 \tcost :  2.260271\n",
      "step :  9600 \tcost :  2.260271\n",
      "step :  9800 \tcost :  2.260271\n",
      "step :  10000 \tcost :  2.260271\n"
     ]
    }
   ],
   "source": [
    "for t in range(10001):    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    cost = criterion(y_pred,torch.max(Y, 1)[1])\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if t % 200 == 0:\n",
    "        print(\"step : \",t,\"\\tcost : \",cost.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-normalized input\n",
    "\n",
    "> 우리가 learning rate를 매우 적절히 줘도 nan값이 나올때가 있다. 그럴때는 normalized가 진행되지 않은 것은 아닌지 확인해보아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "X = Variable(torch.Tensor(x_data))\n",
    "Y = Variable(torch.Tensor(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Choi-seonyeol/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(4,1,bias = True) # 2 in and 1 out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4)\n",
    "        y_pred = self.linear(x)\n",
    "        F.softmax(y_pred,dim=1)\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 \tcost :  2044810000000.0\n",
      "step :  200 \tcost :  nan\n",
      "step :  400 \tcost :  nan\n",
      "step :  600 \tcost :  nan\n",
      "step :  800 \tcost :  nan\n",
      "step :  1000 \tcost :  nan\n",
      "step :  1200 \tcost :  nan\n",
      "step :  1400 \tcost :  nan\n",
      "step :  1600 \tcost :  nan\n",
      "step :  1800 \tcost :  nan\n",
      "step :  2000 \tcost :  nan\n",
      "step :  2200 \tcost :  nan\n",
      "step :  2400 \tcost :  nan\n",
      "step :  2600 \tcost :  nan\n",
      "step :  2800 \tcost :  nan\n",
      "step :  3000 \tcost :  nan\n",
      "step :  3200 \tcost :  nan\n",
      "step :  3400 \tcost :  nan\n",
      "step :  3600 \tcost :  nan\n",
      "step :  3800 \tcost :  nan\n",
      "step :  4000 \tcost :  nan\n",
      "step :  4200 \tcost :  nan\n",
      "step :  4400 \tcost :  nan\n",
      "step :  4600 \tcost :  nan\n",
      "step :  4800 \tcost :  nan\n",
      "step :  5000 \tcost :  nan\n",
      "step :  5200 \tcost :  nan\n",
      "step :  5400 \tcost :  nan\n",
      "step :  5600 \tcost :  nan\n",
      "step :  5800 \tcost :  nan\n",
      "step :  6000 \tcost :  nan\n",
      "step :  6200 \tcost :  nan\n",
      "step :  6400 \tcost :  nan\n",
      "step :  6600 \tcost :  nan\n",
      "step :  6800 \tcost :  nan\n",
      "step :  7000 \tcost :  nan\n",
      "step :  7200 \tcost :  nan\n",
      "step :  7400 \tcost :  nan\n",
      "step :  7600 \tcost :  nan\n",
      "step :  7800 \tcost :  nan\n",
      "step :  8000 \tcost :  nan\n",
      "step :  8200 \tcost :  nan\n",
      "step :  8400 \tcost :  nan\n",
      "step :  8600 \tcost :  nan\n",
      "step :  8800 \tcost :  nan\n",
      "step :  9000 \tcost :  nan\n",
      "step :  9200 \tcost :  nan\n",
      "step :  9400 \tcost :  nan\n",
      "step :  9600 \tcost :  nan\n",
      "step :  9800 \tcost :  nan\n",
      "step :  10000 \tcost :  nan\n"
     ]
    }
   ],
   "source": [
    "for t in range(10001):    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    cost = criterion(y_pred,Y)\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if t % 200 == 0:\n",
    "        print(\"step : \",t,\"\\tcost : \",cost.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제가 발생하는 것을 확인 할 수 있다.\n",
    "이는 Normalize가 되지 않아서 발생하는 문제로,\n",
    "```\n",
    "xy = MinMaxScaler(xy)\n",
    "```\n",
    "를 통해 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881783 0.83755792]\n",
      " [0.54412549 0.50274824 0.57608696 0.60646801 0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.4258239  0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.4258239  0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "xy = MinMaxScaler().fit_transform(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "X = Variable(torch.Tensor(x_data))\n",
    "Y = Variable(torch.Tensor(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Choi-seonyeol/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(4,1,bias = True) # 2 in and 1 out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4)\n",
    "        y_pred = self.linear(x)\n",
    "        F.softmax(y_pred,dim=1)\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "criterion = nn.MSELoss(size_average = False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 \tcost :  8.311975\n",
      "step :  200 \tcost :  0.027330775\n",
      "step :  400 \tcost :  0.026763298\n",
      "step :  600 \tcost :  0.026541281\n",
      "step :  800 \tcost :  0.02634779\n",
      "step :  1000 \tcost :  0.026175305\n",
      "step :  1200 \tcost :  0.026020056\n",
      "step :  1400 \tcost :  0.02587903\n",
      "step :  1600 \tcost :  0.0257498\n",
      "step :  1800 \tcost :  0.025630413\n",
      "step :  2000 \tcost :  0.02551933\n",
      "step :  2200 \tcost :  0.025415236\n",
      "step :  2400 \tcost :  0.025317142\n",
      "step :  2600 \tcost :  0.02522422\n",
      "step :  2800 \tcost :  0.025135778\n",
      "step :  3000 \tcost :  0.025051285\n",
      "step :  3200 \tcost :  0.024970278\n",
      "step :  3400 \tcost :  0.0248924\n",
      "step :  3600 \tcost :  0.024817368\n",
      "step :  3800 \tcost :  0.024744868\n",
      "step :  4000 \tcost :  0.02467478\n",
      "step :  4200 \tcost :  0.024606826\n",
      "step :  4400 \tcost :  0.024540981\n",
      "step :  4600 \tcost :  0.024477035\n",
      "step :  4800 \tcost :  0.024414888\n",
      "step :  5000 \tcost :  0.024354486\n",
      "step :  5200 \tcost :  0.024295727\n",
      "step :  5400 \tcost :  0.024238536\n",
      "step :  5600 \tcost :  0.024182871\n",
      "step :  5800 \tcost :  0.024128618\n",
      "step :  6000 \tcost :  0.024075806\n",
      "step :  6200 \tcost :  0.024024324\n",
      "step :  6400 \tcost :  0.02397415\n",
      "step :  6600 \tcost :  0.023925282\n",
      "step :  6800 \tcost :  0.023877615\n",
      "step :  7000 \tcost :  0.023831133\n",
      "step :  7200 \tcost :  0.023785844\n",
      "step :  7400 \tcost :  0.023741672\n",
      "step :  7600 \tcost :  0.023698598\n",
      "step :  7800 \tcost :  0.023656592\n",
      "step :  8000 \tcost :  0.023615628\n",
      "step :  8200 \tcost :  0.023575697\n",
      "step :  8400 \tcost :  0.023536742\n",
      "step :  8600 \tcost :  0.023498766\n",
      "step :  8800 \tcost :  0.02346171\n",
      "step :  9000 \tcost :  0.023425581\n",
      "step :  9200 \tcost :  0.023390343\n",
      "step :  9400 \tcost :  0.023355993\n",
      "step :  9600 \tcost :  0.023322469\n",
      "step :  9800 \tcost :  0.023289781\n",
      "step :  10000 \tcost :  0.023257904\n"
     ]
    }
   ],
   "source": [
    "for t in range(10001):    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    cost = criterion(y_pred,Y)\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if t % 200 == 0:\n",
    "        print(\"step : \",t,\"\\tcost : \",cost.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
