{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB02_Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis and cost function\n",
    "$$H(x) = Wx + b$$\n",
    "$$ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)})-y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(x) = Wx+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X and Y data\n",
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "## 여기서, Variable 의 의미는 우리가 쓰는 ' 변수'의 의미와는 다르다. 여기서 말하는 Variable 이란,\n",
    "## \"TensorFlow가 학습을 통해 변경시켜야 하는 값\" 정도로 이해하면 된다. 즉, Trainable !한 값\n",
    "## Variable은 첫번째로, 그 값의 shape을 정의해주고 -> 여기서는 처음에 랜덤값으로 1의 shape으로 정의 즉, 하나의 값\n",
    "## 초기값은 랜덤으로 줬음\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ cost(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)})-y^{(i)})^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "## 아직까지는 이 내용이 어렵더라도, 그저 Magic으로 생각하자! 값을 최적화하는 방법은, Gradient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.27343324 [0.3659897] [1.3418708]\n",
      "20 0.24034981 [0.427207] [1.29263]\n",
      "40 0.21821712 [0.45711806] [1.2331972]\n",
      "60 0.19818754 [0.4829164] [1.1753666]\n",
      "80 0.1799972 [0.5072445] [1.1201408]\n",
      "100 0.16347633 [0.53040475] [1.0674994]\n",
      "120 0.14847179 [0.5524743] [1.017331]\n",
      "140 0.1348444 [0.5735065] [0.9695201]\n",
      "160 0.122467935 [0.59354997] [0.9239564]\n",
      "180 0.111227356 [0.61265165] [0.8805338]\n",
      "200 0.10101843 [0.6308556] [0.83915204]\n",
      "220 0.09174654 [0.6482041] [0.79971486]\n",
      "240 0.08332571 [0.6647371] [0.7621313]\n",
      "260 0.075677745 [0.68049324] [0.7263138]\n",
      "280 0.06873174 [0.6955089] [0.69217974]\n",
      "300 0.062423196 [0.70981896] [0.6596498]\n",
      "320 0.05669378 [0.7234563] [0.6286487]\n",
      "340 0.05149019 [0.7364529] [0.5991046]\n",
      "360 0.04676423 [0.74883866] [0.5709488]\n",
      "380 0.042471986 [0.76064235] [0.5441162]\n",
      "400 0.038573757 [0.77189124] [0.51854473]\n",
      "420 0.035033304 [0.78261155] [0.49417514]\n",
      "440 0.03181782 [0.792828] [0.4709507]\n",
      "460 0.028897462 [0.8025643] [0.44881773]\n",
      "480 0.026245127 [0.8118431] [0.42772493]\n",
      "500 0.02383625 [0.82068574] [0.40762338]\n",
      "520 0.021648431 [0.8291128] [0.38846657]\n",
      "540 0.01966144 [0.837144] [0.37021005]\n",
      "560 0.017856857 [0.8447976] [0.35281155]\n",
      "580 0.01621787 [0.8520916] [0.33623064]\n",
      "600 0.0147293145 [0.85904276] [0.32042903]\n",
      "620 0.013377414 [0.8656672] [0.30537003]\n",
      "640 0.012149579 [0.8719804] [0.29101875]\n",
      "660 0.011034447 [0.8779967] [0.277342]\n",
      "680 0.010021658 [0.88373053] [0.26430795]\n",
      "700 0.009101834 [0.8891948] [0.2518864]\n",
      "720 0.008266439 [0.8944022] [0.24004866]\n",
      "740 0.007507697 [0.89936495] [0.22876722]\n",
      "760 0.006818619 [0.9040944] [0.21801601]\n",
      "780 0.0061927824 [0.90860164] [0.20777006]\n",
      "800 0.0056243725 [0.91289693] [0.19800562]\n",
      "820 0.005108152 [0.91699046] [0.1887001]\n",
      "840 0.0046393047 [0.92089164] [0.17983189]\n",
      "860 0.0042134845 [0.9246095] [0.17138045]\n",
      "880 0.003826753 [0.9281526] [0.16332616]\n",
      "900 0.0034755152 [0.93152916] [0.15565039]\n",
      "920 0.003156526 [0.934747] [0.1483354]\n",
      "940 0.0028668123 [0.9378136] [0.14136422]\n",
      "960 0.0026036873 [0.9407362] [0.13472065]\n",
      "980 0.0023647018 [0.9435213] [0.12838928]\n",
      "1000 0.002147657 [0.94617563] [0.12235545]\n",
      "1020 0.0019505379 [0.94870526] [0.11660517]\n",
      "1040 0.0017715093 [0.95111585] [0.11112512]\n",
      "1060 0.0016089153 [0.95341325] [0.10590264]\n",
      "1080 0.0014612395 [0.95560265] [0.1009256]\n",
      "1100 0.0013271241 [0.95768917] [0.09618247]\n",
      "1120 0.0012053197 [0.9596776] [0.09166227]\n",
      "1140 0.0010946888 [0.96157265] [0.0873545]\n",
      "1160 0.0009942089 [0.96337855] [0.08324913]\n",
      "1180 0.00090295885 [0.96509963] [0.07933673]\n",
      "1200 0.0008200814 [0.96673983] [0.07560819]\n",
      "1220 0.0007448106 [0.96830297] [0.07205488]\n",
      "1240 0.0006764491 [0.9697925] [0.06866859]\n",
      "1260 0.0006143631 [0.9712122] [0.06544144]\n",
      "1280 0.0005579724 [0.9725652] [0.06236592]\n",
      "1300 0.0005067627 [0.9738545] [0.05943495]\n",
      "1320 0.00046024853 [0.97508323] [0.05664174]\n",
      "1340 0.00041800554 [0.97625417] [0.0539798]\n",
      "1360 0.00037964186 [0.97737014] [0.05144296]\n",
      "1380 0.00034479672 [0.9784336] [0.04902536]\n",
      "1400 0.00031314787 [0.9794471] [0.04672139]\n",
      "1420 0.00028440825 [0.98041314] [0.04452568]\n",
      "1440 0.00025830485 [0.9813336] [0.04243314]\n",
      "1460 0.00023459653 [0.9822108] [0.04043894]\n",
      "1480 0.00021306389 [0.9830468] [0.03853846]\n",
      "1500 0.00019350696 [0.9838436] [0.03672729]\n",
      "1520 0.00017574609 [0.98460287] [0.03500123]\n",
      "1540 0.00015961604 [0.9853265] [0.0333563]\n",
      "1560 0.00014496535 [0.9860161] [0.03178865]\n",
      "1580 0.00013166074 [0.9866733] [0.03029471]\n",
      "1600 0.00011957553 [0.9872996] [0.02887095]\n",
      "1620 0.00010859961 [0.9878965] [0.02751412]\n",
      "1640 9.863233e-05 [0.9884653] [0.02622106]\n",
      "1660 8.958075e-05 [0.98900735] [0.02498876]\n",
      "1680 8.135732e-05 [0.989524] [0.02381439]\n",
      "1700 7.389029e-05 [0.9900164] [0.02269519]\n",
      "1720 6.710782e-05 [0.9904856] [0.02162856]\n",
      "1740 6.0948394e-05 [0.99093276] [0.02061205]\n",
      "1760 5.535345e-05 [0.9913589] [0.01964333]\n",
      "1780 5.0273284e-05 [0.991765] [0.01872015]\n",
      "1800 4.5658264e-05 [0.99215204] [0.01784034]\n",
      "1820 4.146803e-05 [0.9925208] [0.01700191]\n",
      "1840 3.7661754e-05 [0.99287236] [0.01620288]\n",
      "1860 3.4205652e-05 [0.99320734] [0.0154414]\n",
      "1880 3.106592e-05 [0.9935265] [0.01471571]\n",
      "1900 2.8214266e-05 [0.99383074] [0.01402415]\n",
      "1920 2.5625044e-05 [0.99412066] [0.01336508]\n",
      "1940 2.3273244e-05 [0.9943969] [0.01273698]\n",
      "1960 2.113721e-05 [0.99466026] [0.0121384]\n",
      "1980 1.9196821e-05 [0.9949112] [0.01156798]\n",
      "2000 1.7435377e-05 [0.9951503] [0.01102435]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initialized global variables in the gragh.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# 이걸 해주어야, 전역변수로서 tensor의 변수를 사용할 수 있다!! 꼭 기억.아까 variable만들어준거 기억\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    sess.run(train) # 방금 만든, optimizer에서 cost를 최소화 하는 방향으로 가겠다고 정의한거\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost),sess.run(W),sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 19.492598 [0.65120566] [-1.236138]\n",
      "20 0.6927371 [1.5342059] [-0.84489274]\n",
      "40 0.60463923 [1.5031054] [-0.716444]\n",
      "60 0.52803594 [1.4701742] [-0.59747875]\n",
      "80 0.4611376 [1.4393822] [-0.48630935]\n",
      "100 0.4027149 [1.4106067] [-0.3824207]\n",
      "120 0.35169396 [1.3837156] [-0.28533578]\n",
      "140 0.30713677 [1.3585858] [-0.19460887]\n",
      "160 0.2682249 [1.3351018] [-0.10982391]\n",
      "180 0.23424277 [1.3131557] [-0.0305916]\n",
      "200 0.20456596 [1.2926469] [0.04345179]\n",
      "220 0.17864887 [1.2734811] [0.11264599]\n",
      "240 0.1560155 [1.2555707] [0.17730862]\n",
      "260 0.1362494 [1.2388331] [0.23773643]\n",
      "280 0.1189877 [1.2231917] [0.2942068]\n",
      "300 0.103912786 [1.2085748] [0.34697887]\n",
      "320 0.09074781 [1.1949149] [0.39629483]\n",
      "340 0.07925077 [1.1821499] [0.44238102]\n",
      "360 0.06921029 [1.1702207] [0.48544896]\n",
      "380 0.06044185 [1.1590729] [0.5256966]\n",
      "400 0.05278433 [1.1486549] [0.5633082]\n",
      "420 0.046096906 [1.1389195] [0.5984566]\n",
      "440 0.040256757 [1.1298215] [0.63130313]\n",
      "460 0.035156492 [1.1213194] [0.6619985]\n",
      "480 0.030702436 [1.1133741] [0.69068354]\n",
      "500 0.026812648 [1.1059492] [0.7174902]\n",
      "520 0.023415677 [1.0990103] [0.74254113]\n",
      "540 0.020449098 [1.0925261] [0.76595145]\n",
      "560 0.01785832 [1.0864664] [0.7878285]\n",
      "580 0.0155958235 [1.0808038] [0.8082729]\n",
      "600 0.013619954 [1.0755118] [0.82737833]\n",
      "620 0.011894381 [1.0705664] [0.8452326]\n",
      "640 0.01038746 [1.065945] [0.86191756]\n",
      "660 0.009071445 [1.0616262] [0.87750965]\n",
      "680 0.007922154 [1.0575902] [0.89208084]\n",
      "700 0.0069184764 [1.0538186] [0.90569776]\n",
      "720 0.006041949 [1.0502939] [0.9184228]\n",
      "740 0.0052764844 [1.0470002] [0.9303144]\n",
      "760 0.0046079895 [1.0439221] [0.9414273]\n",
      "780 0.0040241955 [1.0410458] [0.95181227]\n",
      "800 0.0035143457 [1.0383575] [0.9615172]\n",
      "820 0.0030691107 [1.0358454] [0.97058666]\n",
      "840 0.0026802632 [1.0334978] [0.979062]\n",
      "860 0.0023407054 [1.0313041] [0.98698235]\n",
      "880 0.002044153 [1.029254] [0.99438393]\n",
      "900 0.0017851818 [1.027338] [1.0013007]\n",
      "920 0.0015590062 [1.0255477] [1.0077648]\n",
      "940 0.0013614859 [1.0238746] [1.0138053]\n",
      "960 0.0011890066 [1.0223111] [1.0194501]\n",
      "980 0.0010383709 [1.02085] [1.0247252]\n",
      "1000 0.00090681313 [1.0194845] [1.029655]\n",
      "1020 0.0007919323 [1.0182084] [1.034262]\n",
      "1040 0.0006916009 [1.0170159] [1.038567]\n",
      "1060 0.0006039807 [1.0159016] [1.0425903]\n",
      "1080 0.00052746374 [1.0148602] [1.04635]\n",
      "1100 0.0004606393 [1.013887] [1.0498635]\n",
      "1120 0.0004022792 [1.0129775] [1.053147]\n",
      "1140 0.00035131996 [1.0121276] [1.0562153]\n",
      "1160 0.00030681197 [1.0113335] [1.0590824]\n",
      "1180 0.0002679425 [1.0105913] [1.0617622]\n",
      "1200 0.00023399542 [1.0098976] [1.0642663]\n",
      "1220 0.00020434693 [1.0092493] [1.0666066]\n",
      "1240 0.00017845735 [1.0086436] [1.0687935]\n",
      "1260 0.00015585446 [1.0080776] [1.0708371]\n",
      "1280 0.00013610497 [1.0075486] [1.072747]\n",
      "1300 0.000118863376 [1.0070542] [1.0745319]\n",
      "1320 0.000103803235 [1.0065923] [1.0761999]\n",
      "1340 9.065302e-05 [1.0061605] [1.0777584]\n",
      "1360 7.916724e-05 [1.0057571] [1.0792149]\n",
      "1380 6.9138e-05 [1.00538] [1.0805763]\n",
      "1400 6.037841e-05 [1.0050277] [1.0818484]\n",
      "1420 5.2730124e-05 [1.0046984] [1.0830371]\n",
      "1440 4.6047266e-05 [1.0043907] [1.084148]\n",
      "1460 4.0214687e-05 [1.0041032] [1.0851862]\n",
      "1480 3.512003e-05 [1.0038345] [1.0861562]\n",
      "1500 3.067022e-05 [1.0035834] [1.0870628]\n",
      "1520 2.6784724e-05 [1.0033486] [1.0879102]\n",
      "1540 2.339148e-05 [1.0031294] [1.0887021]\n",
      "1560 2.0428037e-05 [1.0029244] [1.089442]\n",
      "1580 1.7839373e-05 [1.0027329] [1.0901334]\n",
      "1600 1.5579222e-05 [1.0025538] [1.0907795]\n",
      "1620 1.36049475e-05 [1.0023866] [1.0913835]\n",
      "1640 1.1881803e-05 [1.0022303] [1.0919478]\n",
      "1660 1.0375725e-05 [1.0020843] [1.0924753]\n",
      "1680 9.060643e-06 [1.0019478] [1.0929681]\n",
      "1700 7.9135225e-06 [1.0018201] [1.0934286]\n",
      "1720 6.91062e-06 [1.001701] [1.0938591]\n",
      "1740 6.03499e-06 [1.0015895] [1.0942612]\n",
      "1760 5.2705095e-06 [1.0014855] [1.094637]\n",
      "1780 4.602398e-06 [1.0013881] [1.0949885]\n",
      "1800 4.0192267e-06 [1.0012972] [1.0953166]\n",
      "1820 3.510222e-06 [1.0012122] [1.0956235]\n",
      "1840 3.0651518e-06 [1.0011328] [1.0959101]\n",
      "1860 2.6772905e-06 [1.0010587] [1.0961779]\n",
      "1880 2.3379353e-06 [1.0009894] [1.0964282]\n",
      "1900 2.041848e-06 [1.0009245] [1.0966622]\n",
      "1920 1.7831435e-06 [1.000864] [1.0968807]\n",
      "1940 1.5573654e-06 [1.0008075] [1.0970848]\n",
      "1960 1.3602512e-06 [1.0007545] [1.0972757]\n",
      "1980 1.1876948e-06 [1.0007052] [1.097454]\n",
      "2000 1.0374654e-06 [1.000659] [1.0976207]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([1]),name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name = 'bias')\n",
    "X = tf.placeholder(tf.float32, shape = [None])\n",
    "Y = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initialized global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# 만약 이걸 안해주면, 학습의 결과가 중첩돼서 적용되지 않고, 첫번째 값만 도출 되겠지.\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val,_ = \\\n",
    "        sess.run([cost,W,b,train],\n",
    "                feed_dict = {X:[1,2,3,4,5],Y:[2.1,3.1,4.1,5.1,6.1]})\n",
    "    if step % 20 == 0:\n",
    "        print(step,cost_val,W_val,b_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.100916]\n",
      "[3.5992682]\n",
      "[2.5986092 4.599927 ]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hypothesis, feed_dict={X:[5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
